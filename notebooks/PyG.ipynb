{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erikh/GNN_test/gnn-venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "design = pd.read_csv('../data/design_matrix_group_de.tsv', sep=\"\\t\")\n",
    "samples = design['sample'].tolist()\n",
    "y = np.array(design['group'])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent</th>\n",
       "      <th>child</th>\n",
       "      <th>TM_M2012_010</th>\n",
       "      <th>TM_M2012_011</th>\n",
       "      <th>TM_M2012_012</th>\n",
       "      <th>TM_M2012_013</th>\n",
       "      <th>TM_M2012_014</th>\n",
       "      <th>TM_M2012_016</th>\n",
       "      <th>TM_M2012_017</th>\n",
       "      <th>TM_M2012_018</th>\n",
       "      <th>...</th>\n",
       "      <th>TM_M2012_190</th>\n",
       "      <th>TM_M2012_191</th>\n",
       "      <th>TM_M2012_192</th>\n",
       "      <th>TM_M2012_196</th>\n",
       "      <th>TM_M2012_197</th>\n",
       "      <th>TM_M2012_198</th>\n",
       "      <th>TM_M2012_199</th>\n",
       "      <th>TM_M2012_200</th>\n",
       "      <th>TM_M2012_202</th>\n",
       "      <th>TM_M2012_203</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apoptosis</td>\n",
       "      <td>Intrinsic Pathway for Apoptosis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apoptosis</td>\n",
       "      <td>Regulation of Apoptosis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apoptosis</td>\n",
       "      <td>Caspase activation via extrinsic apoptotic sig...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apoptosis</td>\n",
       "      <td>Apoptotic execution phase</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hemostasis</td>\n",
       "      <td>Formation of Fibrin Clot (Clotting Cascade)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8267</th>\n",
       "      <td>Scavenging by Class F Receptors</td>\n",
       "      <td>HYOU1_HUMAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.381515</td>\n",
       "      <td>1.822727</td>\n",
       "      <td>2.347224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.658090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.790136</td>\n",
       "      <td>2.466573</td>\n",
       "      <td>1.760683</td>\n",
       "      <td>1.861283</td>\n",
       "      <td>1.963151</td>\n",
       "      <td>2.017270</td>\n",
       "      <td>2.026863</td>\n",
       "      <td>1.951526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268</th>\n",
       "      <td>XBP1(S) activates chaperone genes</td>\n",
       "      <td>HYOU1_HUMAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.381515</td>\n",
       "      <td>1.822727</td>\n",
       "      <td>2.347224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.658090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.790136</td>\n",
       "      <td>2.466573</td>\n",
       "      <td>1.760683</td>\n",
       "      <td>1.861283</td>\n",
       "      <td>1.963151</td>\n",
       "      <td>2.017270</td>\n",
       "      <td>2.026863</td>\n",
       "      <td>1.951526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8269</th>\n",
       "      <td>Assembly of active LPL and LIPC lipase complexes</td>\n",
       "      <td>ANGL3_HUMAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.759549</td>\n",
       "      <td>1.777595</td>\n",
       "      <td>1.675426</td>\n",
       "      <td>1.848508</td>\n",
       "      <td>1.756070</td>\n",
       "      <td>1.701783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.740658</td>\n",
       "      <td>1.711269</td>\n",
       "      <td>1.687311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693959</td>\n",
       "      <td>1.830008</td>\n",
       "      <td>1.947588</td>\n",
       "      <td>1.984524</td>\n",
       "      <td>1.846057</td>\n",
       "      <td>1.873182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8270</th>\n",
       "      <td>NR1H2 &amp; NR1H3 regulate gene expression linked ...</td>\n",
       "      <td>ANGL3_HUMAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.759549</td>\n",
       "      <td>1.777595</td>\n",
       "      <td>1.675426</td>\n",
       "      <td>1.848508</td>\n",
       "      <td>1.756070</td>\n",
       "      <td>1.701783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.740658</td>\n",
       "      <td>1.711269</td>\n",
       "      <td>1.687311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693959</td>\n",
       "      <td>1.830008</td>\n",
       "      <td>1.947588</td>\n",
       "      <td>1.984524</td>\n",
       "      <td>1.846057</td>\n",
       "      <td>1.873182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8271</th>\n",
       "      <td>Serine biosynthesis</td>\n",
       "      <td>SERC_HUMAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.948843</td>\n",
       "      <td>...</td>\n",
       "      <td>2.105019</td>\n",
       "      <td>2.110619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.090983</td>\n",
       "      <td>2.215001</td>\n",
       "      <td>2.322494</td>\n",
       "      <td>2.334094</td>\n",
       "      <td>2.347952</td>\n",
       "      <td>2.172224</td>\n",
       "      <td>2.282632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8272 rows Ã— 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 parent  \\\n",
       "0                                             Apoptosis   \n",
       "1                                             Apoptosis   \n",
       "2                                             Apoptosis   \n",
       "3                                             Apoptosis   \n",
       "4                                            Hemostasis   \n",
       "...                                                 ...   \n",
       "8267                    Scavenging by Class F Receptors   \n",
       "8268                  XBP1(S) activates chaperone genes   \n",
       "8269   Assembly of active LPL and LIPC lipase complexes   \n",
       "8270  NR1H2 & NR1H3 regulate gene expression linked ...   \n",
       "8271                                Serine biosynthesis   \n",
       "\n",
       "                                                  child  TM_M2012_010  \\\n",
       "0                       Intrinsic Pathway for Apoptosis           0.0   \n",
       "1                               Regulation of Apoptosis           0.0   \n",
       "2     Caspase activation via extrinsic apoptotic sig...           0.0   \n",
       "3                             Apoptotic execution phase           0.0   \n",
       "4           Formation of Fibrin Clot (Clotting Cascade)           0.0   \n",
       "...                                                 ...           ...   \n",
       "8267                                        HYOU1_HUMAN           0.0   \n",
       "8268                                        HYOU1_HUMAN           0.0   \n",
       "8269                                        ANGL3_HUMAN           0.0   \n",
       "8270                                        ANGL3_HUMAN           0.0   \n",
       "8271                                         SERC_HUMAN           0.0   \n",
       "\n",
       "      TM_M2012_011  TM_M2012_012  TM_M2012_013  TM_M2012_014  TM_M2012_016  \\\n",
       "0         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "2         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "3         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "4         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "8267      0.000000      0.000000      2.381515      1.822727      2.347224   \n",
       "8268      0.000000      0.000000      2.381515      1.822727      2.347224   \n",
       "8269      1.759549      1.777595      1.675426      1.848508      1.756070   \n",
       "8270      1.759549      1.777595      1.675426      1.848508      1.756070   \n",
       "8271      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "      TM_M2012_017  TM_M2012_018  ...  TM_M2012_190  TM_M2012_191  \\\n",
       "0         0.000000      0.000000  ...      0.000000      0.000000   \n",
       "1         0.000000      0.000000  ...      0.000000      0.000000   \n",
       "2         0.000000      0.000000  ...      0.000000      0.000000   \n",
       "3         0.000000      0.000000  ...      0.000000      0.000000   \n",
       "4         0.000000      0.000000  ...      0.000000      0.000000   \n",
       "...            ...           ...  ...           ...           ...   \n",
       "8267      0.000000      0.000000  ...      1.658090      0.000000   \n",
       "8268      0.000000      0.000000  ...      1.658090      0.000000   \n",
       "8269      1.701783      0.000000  ...      1.740658      1.711269   \n",
       "8270      1.701783      0.000000  ...      1.740658      1.711269   \n",
       "8271      0.000000      1.948843  ...      2.105019      2.110619   \n",
       "\n",
       "      TM_M2012_192  TM_M2012_196  TM_M2012_197  TM_M2012_198  TM_M2012_199  \\\n",
       "0         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "1         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "2         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "3         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "4         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "8267      1.790136      2.466573      1.760683      1.861283      1.963151   \n",
       "8268      1.790136      2.466573      1.760683      1.861283      1.963151   \n",
       "8269      1.687311      0.000000      1.693959      1.830008      1.947588   \n",
       "8270      1.687311      0.000000      1.693959      1.830008      1.947588   \n",
       "8271      0.000000      2.090983      2.215001      2.322494      2.334094   \n",
       "\n",
       "      TM_M2012_200  TM_M2012_202  TM_M2012_203  \n",
       "0         0.000000      0.000000      0.000000  \n",
       "1         0.000000      0.000000      0.000000  \n",
       "2         0.000000      0.000000      0.000000  \n",
       "3         0.000000      0.000000      0.000000  \n",
       "4         0.000000      0.000000      0.000000  \n",
       "...            ...           ...           ...  \n",
       "8267      2.017270      2.026863      1.951526  \n",
       "8268      2.017270      2.026863      1.951526  \n",
       "8269      1.984524      1.846057      1.873182  \n",
       "8270      1.984524      1.846057      1.873182  \n",
       "8271      2.347952      2.172224      2.282632  \n",
       "\n",
       "[8272 rows x 143 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "edgelist = pd.read_csv('../data/processed/sepsis_edgelist_w_values.csv')\n",
    "edgelist.fillna(0, inplace=True)\n",
    "edgelist = edgelist[['parent','child'] + samples]\n",
    "edgelist.set_index(['parent','child'], inplace=True)\n",
    "\n",
    "scaled_features = StandardScaler(with_mean=False).fit_transform(edgelist.values)\n",
    "edgelist = pd.DataFrame(scaled_features, index=edgelist.index, columns=edgelist.columns)\n",
    "edgelist.reset_index(inplace=True)\n",
    "edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2219\n",
      "6960\n"
     ]
    }
   ],
   "source": [
    "all_nodes = list(set(edgelist['child'].values.tolist() + edgelist['parent'].values.tolist()))\n",
    "all_nodes.remove(0)\n",
    "G = nx.DiGraph()\n",
    "for node in all_nodes:\n",
    "    if node in edgelist['child'].values:\n",
    "        node_weights = edgelist[edgelist['child'] == node][samples].values[0]\n",
    "    else:\n",
    "        node_weights = [0]*len(samples)\n",
    "    G.add_node(node, weight = node_weights)\n",
    "\n",
    "for row in edgelist.iterrows():\n",
    "    row = row[1]\n",
    "    source = row['child']\n",
    "    target = row['parent']\n",
    "    if source in G.nodes():\n",
    "        G.add_edge(source, target)\n",
    "    \n",
    "print(G.number_of_nodes())\n",
    "print(G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    1,    2, ..., 2218, 2218, 2218],\n",
       "       [ 564, 1380,  130, ...,  700, 1373, 1972]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_adj = nx.to_pandas_adjacency(G)\n",
    "adj = pd_adj.values\n",
    "edge_index = (adj > 0).nonzero()\n",
    "row, col = edge_index\n",
    "coo = np.array(list(zip(row,col))).T\n",
    "coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 2219\n",
      "Number of edges: 6960\n",
      "Average node degree: 3.14\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "data_list = []\n",
    "for i in range(len(samples)):\n",
    "    sample = samples[i]\n",
    "    x = []\n",
    "    for node in G.nodes():\n",
    "        x.append([G.nodes()[node]['weight'][i]])\n",
    "    data_i = Data(x=torch.tensor(x, dtype=torch.float), edge_index=torch.tensor(coo, dtype=torch.long), y=torch.tensor(y[i], dtype=torch.long))\n",
    "    data_list.append(data_i)\n",
    "    \n",
    "dataloader = DataLoader(data_list, shuffle=True, batch_size=16, drop_last=True)\n",
    "dataset = dataloader.dataset\n",
    "data = dataloader.dataset[1]\n",
    "\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (model): Sequential(\n",
       "    (0): GCNConv(-1, 256)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): GCNConv(256, 256)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): GCNConv(256, 256)\n",
       "    (7): ReLU()\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): GCNConv(256, 256)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.5, inplace=False)\n",
       "    (12): GCNConv(256, 256)\n",
       "    (13): ReLU()\n",
       "    (14): Dropout(p=0.5, inplace=False)\n",
       "    (15): <function global_mean_pool at 0x7f3e24a86d30>\n",
       "    (16): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torch.nn import Dropout, Linear, ReLU\n",
    "from torch_geometric.nn import GCNConv, Sequential, global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,  num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.num_classes =  num_classes\n",
    "        # hidden layer node features\n",
    "        self.hidden = 256\n",
    "        self.model = Sequential(\"x, edge_index, batch_index\", [                \n",
    "                (GCNConv(-1, self.hidden), \n",
    "                    \"x, edge_index -> x1\"),\n",
    "                (ReLU(), \"x1 -> x1a\"),                                     \n",
    "                (Dropout(p=0.5), \"x1a -> x1d\"),                                \n",
    "                (GCNConv(self.hidden, self.hidden), \"x1d, edge_index -> x2\"),  \n",
    "                (ReLU(), \"x2 -> x2a\"),                                         \n",
    "                (Dropout(p=0.5), \"x2a -> x2d\"),                                \n",
    "                (GCNConv(self.hidden, self.hidden), \"x2d, edge_index -> x3\"),  \n",
    "                (ReLU(), \"x3 -> x3a\"),                                         \n",
    "                (Dropout(p=0.5), \"x3a -> x3d\"),                               \n",
    "                (GCNConv(self.hidden, self.hidden), \"x3d, edge_index -> x4\"),  \n",
    "                (ReLU(), \"x4 -> x4a\"),                                         \n",
    "                (Dropout(p=0.5), \"x4a -> x4d\"),                                \n",
    "                (GCNConv(self.hidden, self.hidden), \"x4d, edge_index -> x5\"),  \n",
    "                (ReLU(), \"x5 -> x5a\"),                                         \n",
    "                (Dropout(p=0.5), \"x5a -> x5d\"),                                \n",
    "                (global_mean_pool, \"x5d, batch_index -> x6\"),                 \n",
    "                (Linear(self.hidden, self.num_classes), \"x6 -> x_out\")])  \n",
    "        \n",
    "    def forward(self, graph_data):\n",
    "        x, edge_index, batch = graph_data.x, graph_data.edge_index,graph_data.batch\n",
    "        x_out = self.model(x, edge_index, batch)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "model = GCN(num_classes=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6882, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6786, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6722, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6745, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7108, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7187, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6691, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6125, grad_fn=<NllLossBackward0>)\n",
      "Epoch   1 | Loss: 0.61 | Acc: 55.32%\n",
      "tensor(0.6813, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6787, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7067, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6661, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6437, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6739, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6992, grad_fn=<NllLossBackward0>)\n",
      "Epoch   2 | Loss: 0.70 | Acc: 52.48%\n",
      "tensor(0.7062, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6797, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6971, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6948, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6715, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6380, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6351, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7157, grad_fn=<NllLossBackward0>)\n",
      "Epoch   3 | Loss: 0.72 | Acc: 53.19%\n",
      "tensor(0.7697, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6912, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6603, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7197, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6114, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6671, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6146, grad_fn=<NllLossBackward0>)\n",
      "Epoch   4 | Loss: 0.61 | Acc: 52.48%\n",
      "tensor(0.6813, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6867, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7270, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6917, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6991, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6809, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6784, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5817, grad_fn=<NllLossBackward0>)\n",
      "Epoch   5 | Loss: 0.58 | Acc: 51.77%\n",
      "tensor(0.6050, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6490, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7152, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7174, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6685, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7277, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6340, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6809, grad_fn=<NllLossBackward0>)\n",
      "Epoch   6 | Loss: 0.68 | Acc: 52.48%\n",
      "tensor(0.7652, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7047, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6073, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6356, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6789, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7096, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6666, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6440, grad_fn=<NllLossBackward0>)\n",
      "Epoch   7 | Loss: 0.64 | Acc: 50.35%\n",
      "tensor(0.7225, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6436, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6388, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6985, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6792, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7114, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6247, grad_fn=<NllLossBackward0>)\n",
      "Epoch   8 | Loss: 0.62 | Acc: 52.48%\n",
      "tensor(0.6523, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6987, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7143, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6383, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6319, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6617, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6992, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6853, grad_fn=<NllLossBackward0>)\n",
      "Epoch   9 | Loss: 0.69 | Acc: 51.77%\n",
      "tensor(0.6950, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5333, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6963, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6522, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6436, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7067, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7353, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6950, grad_fn=<NllLossBackward0>)\n",
      "Epoch  10 | Loss: 0.69 | Acc: 51.06%\n",
      "tensor(0.7001, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6543, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6776, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7066, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6757, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7038, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5699, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7038, grad_fn=<NllLossBackward0>)\n",
      "Epoch  11 | Loss: 0.70 | Acc: 51.77%\n",
      "tensor(0.6345, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6890, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5556, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6537, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7276, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m      \u001b[39mreturn\u001b[39;00m correct \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(loader\u001b[39m.\u001b[39mdataset)  \u001b[39m# Derive ratio of correct predictions.\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m200\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     26\u001b[0m     acc \u001b[39m=\u001b[39m test(dataloader)\n\u001b[1;32m     27\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m>3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Acc: \u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [8], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, data\u001b[39m.\u001b[39my)  \u001b[39m# Compute the loss.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(loss)\n\u001b[0;32m---> 10\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update parameters based on gradients.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Clear gradients.\u001b[39;00m\n",
      "File \u001b[0;32m~/GNN_test/gnn-venv/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/GNN_test/gnn-venv/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for data in dataloader:  # Iterate in batches over the training dataset.\n",
    "        out = model.forward(data) \n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        print(loss)\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "    return loss\n",
    "         \n",
    "def test(loader):\n",
    "     model.eval()\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "         \n",
    "for epoch in range(1, 200):\n",
    "    loss = train()\n",
    "    acc = test(dataloader)\n",
    "    print(f'Epoch {epoch:>3} | Loss: {loss:.2f} | Acc: {acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be1761451b42faef433011dd807e5a20c9882736cccedab0d2719fa08ffabbe4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
